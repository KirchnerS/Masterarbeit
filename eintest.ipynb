{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1086,
     "status": "ok",
     "timestamp": 1657634470603,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "VLyKQxhoztob"
   },
   "outputs": [],
   "source": [
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel, AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10780,
     "status": "ok",
     "timestamp": 1657633132431,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "XIGa2z3Lz5c6",
    "outputId": "57ee4c0a-d68b-47f5-dcb0-23a1ef0ff847"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1657622873665,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "jWGnJLEbztoe"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "pd.set_option('display.max_rows',500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.set_device(3)\n",
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1075,
     "status": "ok",
     "timestamp": 1657647762999,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "WCzBdMWwztoe",
    "outputId": "b4adcd11-5b75-414f-d9e2-c35d8e530c53"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative    1394\n",
      "neutral     1191\n",
      "positive     611\n",
      "Name: Opinion, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Comment</th>\n",
       "      <th>Opinion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>Die Einigung für Schulen ist fatal. Frau Eisen...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478</th>\n",
       "      <td>Vernünftige Entscheidung, mit der Impfung entf...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2488</th>\n",
       "      <td>Wir sind doch nicht in einer (Standard-) Schul...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>Wie schön, es geht voran.   Bis Ende Mai sind ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>Nun gut, wenn sarkastische Kommentare nicht me...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1066</th>\n",
       "      <td>Übrigens: das RKI prognostizierte am 12. März ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3901</th>\n",
       "      <td>Donny will nach 8 Monaten beginnen Menschen zu...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1393</th>\n",
       "      <td>De lockdau mi sine positive eigeschaft für d'u...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3027</th>\n",
       "      <td>Deutschlands \"Intelligentia\" regelt ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2576</th>\n",
       "      <td>Ich kann mich nicht erinnern, dass die Grünen ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3196 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Comment   Opinion\n",
       "401   Die Einigung für Schulen ist fatal. Frau Eisen...  negative\n",
       "478   Vernünftige Entscheidung, mit der Impfung entf...  positive\n",
       "2488  Wir sind doch nicht in einer (Standard-) Schul...   neutral\n",
       "843   Wie schön, es geht voran.   Bis Ende Mai sind ...  positive\n",
       "344   Nun gut, wenn sarkastische Kommentare nicht me...   neutral\n",
       "...                                                 ...       ...\n",
       "1066  Übrigens: das RKI prognostizierte am 12. März ...   neutral\n",
       "3901  Donny will nach 8 Monaten beginnen Menschen zu...   neutral\n",
       "1393  De lockdau mi sine positive eigeschaft für d'u...  negative\n",
       "3027            Deutschlands \"Intelligentia\" regelt ...  negative\n",
       "2576  Ich kann mich nicht erinnern, dass die Grünen ...   neutral\n",
       "\n",
       "[3196 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Here we set the labels of the topics, in this case we have 7 labels for comments and 8 for articles\n",
    "# label_to_id = {\"politik\" : 0, \"maßnahmen\" : 1, \"infektion\" : 2, \"impfung\": 3, \"lockdown\": 4, \"wirtschaft\":5, \"lockerung\":6, \"überblick\": 7}\n",
    "# label_to_id = {\"politik\" : 0, \"maßnahmen\" : 1, \"infektion\" : 2, \"impfung\": 3, \"lockdown\": 4, \"wirtschaft\":5, \"lockerung\":6}\n",
    "label_to_id = {\"positive\":0, \"negative\" : 1, \"neutral\": 2}\n",
    "# dataset_topic = pd.read_csv(\"annotated_data/annotated_data_with_users_and_al_cleanedArticleText.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"Topic_comment\", \"Topic_article\", \"Comment\", \"Method\"], encoding=\"utf-8\")\n",
    "dataset_topic = pd.read_csv(\"annotated_data/annotated_data_with_users_and_al_cleaned2check.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"Topic_comment\", \"Topic_article\", \"Comment\", \"Method\"], encoding=\"ISO-8859-1\")\n",
    "dataset_topic = dataset_topic.loc[dataset_topic[\"Opinion\"].isin(label_to_id.keys())]\n",
    "# dataset_topic = dataset_topic.loc[dataset_topic[\"Topic_comment\"].isin(label_to_id.keys())]\n",
    "# dataset_topic = dataset_topic.loc[dataset_topic[\"Topic_article\"].isin(label_to_id.keys())]\n",
    "# dataset_topic = dataset_topic[[\"Comment\", \"Topic_comment\"]]\n",
    "dataset_topic = dataset_topic[[\"Comment\", \"Opinion\"]]\n",
    "# dataset_topic = dataset_topic[[\"Comment\", \"Topic_article\"]]\n",
    "# dataset_topic = dataset_topic.loc[dataset_topic[\"Topic_article\"].isin(label_to_id.keys())]\n",
    "# dataset_topic = dataset_topic.loc[dataset_topic[\"Topic_comment\"].isin(label_to_id.keys())]\n",
    "dataset_topic = dataset_topic.loc[dataset_topic[\"Opinion\"].isin(label_to_id.keys())]\n",
    "# dataset_topic.columns = [\"Comment\", \"Topic\"]\n",
    "dataset_topic.columns = [\"Comment\", \"Opinion\"]\n",
    "df_train, df_test = np.split(dataset_topic.sample(frac=1, random_state=77), [int(.8*len(dataset_topic))])\n",
    "\n",
    "\n",
    "\n",
    "len(dataset_topic)\n",
    "dataset_topic.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_train.drop(df_train.loc[df_train[\"Comment\"].isnull()].index, axis = 0, inplace = True)\n",
    "# for x in df_train[\"Comment\"]:\n",
    "#     if type(x) != str:\n",
    "#         print(type(x), x)\n",
    "print(df_train[\"Opinion\"].value_counts())\n",
    "df_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(x)\n\u001b[1;32m      8\u001b[0m request \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.spiegel.de/wissenschaft/medizin/corona-news-am-samstag-die-wichtigsten-entwicklungen-zu-sars-cov-2-und-covid-19-a-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39mdataset_topic\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28mint\u001b[39m(x), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m----> 9\u001b[0m soup \u001b[38;5;241m=\u001b[39m \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhtml.parser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m title \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m     11\u001b[0m subtitle \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mproperty\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mog:description\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:333\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39minitialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 333\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    334\u001b[0m     success \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:451\u001b[0m, in \u001b[0;36mBeautifulSoup._feed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Convert the document to Unicode.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/bs4/builder/_htmlparser.py:399\u001b[0m, in \u001b[0;36mHTMLParserTreeBuilder.feed\u001b[0;34m(self, markup)\u001b[0m\n\u001b[1;32m    397\u001b[0m parser\u001b[38;5;241m.\u001b[39msoup \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msoup\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    400\u001b[0m     parser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTMLParseError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/html/parser.py:110\u001b[0m, in \u001b[0;36mHTMLParser.feed\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrawdata \u001b[38;5;241m+\u001b[39m data\n\u001b[0;32m--> 110\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/html/parser.py:172\u001b[0m, in \u001b[0;36mHTMLParser.goahead\u001b[0;34m(self, end)\u001b[0m\n\u001b[1;32m    170\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_starttag(i)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m</\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[0;32m--> 172\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<!--\u001b[39m\u001b[38;5;124m\"\u001b[39m, i):\n\u001b[1;32m    174\u001b[0m     k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_comment(i)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/html/parser.py:420\u001b[0m, in \u001b[0;36mHTMLParser.parse_endtag\u001b[0;34m(self, i)\u001b[0m\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle_data(rawdata[i:gtpos])\n\u001b[1;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m gtpos\n\u001b[0;32m--> 420\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43melem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclear_cdata_mode()\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gtpos\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/bs4/builder/_htmlparser.py:193\u001b[0m, in \u001b[0;36mBeautifulSoupHTMLParser.handle_endtag\u001b[0;34m(self, name, check_already_closed)\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malready_closed_empty_element\u001b[38;5;241m.\u001b[39mremove(name)\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 193\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_endtag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:743\u001b[0m, in \u001b[0;36mBeautifulSoup.handle_endtag\u001b[0;34m(self, name, nsprefix)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;66;03m#print(\"End tag: \" + name)\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mendData()\n\u001b[0;32m--> 743\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_popToTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsprefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/bs4/__init__.py:686\u001b[0m, in \u001b[0;36mBeautifulSoup._popToTag\u001b[0;34m(self, name, nsprefix, inclusivePop)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    685\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtagStack[i]\n\u001b[0;32m--> 686\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (name \u001b[38;5;241m==\u001b[39m t\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mand\u001b[39;00m nsprefix \u001b[38;5;241m==\u001b[39m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprefix\u001b[49m):\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inclusivePop:\n\u001b[1;32m    688\u001b[0m         most_recently_popped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpopTag()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# This cell scrapes the articles for their header content\n",
    "\n",
    "label_to_id = {\"politik\" : 0, \"maßnahmen\" : 1, \"infektion\" : 2, \"impfung\": 3, \"lockdown\": 4, \"wirtschaft\":5, \"lockerung\":6, \"überblick\":7}\n",
    "dataset_topic = pd.read_csv(\"annotated_data/annotated_data_with_users_and_al_cleaned2check.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"Topic_comment\", \"Topic_article\", \"Comment\", \"Method\"], encoding=\"ISO-8859-1\")\n",
    "dataset_topic = dataset_topic.loc[dataset_topic[\"Topic_article\"].isin(label_to_id.keys())]\n",
    "dataset_topic.reset_index(drop=True, inplace=True)\n",
    "for x in range(len(dataset_topic)):\n",
    "    print(x)\n",
    "\n",
    "    request = requests.get(\"https://www.spiegel.de/wissenschaft/medizin/corona-news-am-samstag-die-wichtigsten-entwicklungen-zu-sars-cov-2-und-covid-19-a-\" +dataset_topic.loc[int(x), \"ID\"])\n",
    "    soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "    title = soup.find(\"title\").text\n",
    "    subtitle = soup.find(\"meta\", property=\"og:description\")[\"content\"]\n",
    "\n",
    "    dataset_topic.loc[x, \"Comment\"] = title + \" \" + subtitle\n",
    "\n",
    "dataset_topic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_topic[\"Comment\"][0]\n",
    "dataset_topic.to_csv(\"annotated_data/annotated_data_with_users_and_al_cleaned2check_article.csv\", encoding=\"utf-8\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "executionInfo": {
     "elapsed": 8771,
     "status": "ok",
     "timestamp": 1657642992816,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "Z0dkvjCeztof"
   },
   "outputs": [],
   "source": [
    "data_text = []\n",
    "data_labels = []\n",
    "text_train = []\n",
    "labels_train = []\n",
    "text_test = np.array([])\n",
    "labels_test = np.array([])\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "\n",
    "\n",
    "\n",
    "# For Topics Article\n",
    "# label_to_id = {\"politik\" : 0, \"maßnahmen\" : 1, \"infektion\" : 2, \"impfung\": 3, \"lockdown\": 4, \"wirtschaft\":5, \"lockerung\":6, \"überblick\":7 }\n",
    "# id_to_label = {0:\"politik\", 1:\"maßnahmen\", 2:\"infektion\", 3: \"impfung\", 4: \"lockdown\", 5:\"wirtschaft\", 6:\"lockerung\", 7:\"überblick\"}\n",
    "\n",
    "\n",
    "# For Topics Comment\n",
    "# label_to_id = {\"politik\" : 0, \"maßnahmen\" : 1, \"infektion\" : 2, \"impfung\": 3, \"lockdown\": 4, \"wirtschaft\":5, \"lockerung\":6}\n",
    "# id_to_label = {0:\"politik\", 1:\"maßnahmen\", 2:\"infektion\", 3: \"impfung\", 4: \"lockdown\", 5:\"wirtschaft\", 6:\"lockerung\"}\n",
    "\n",
    "# For Sentiment\n",
    "label_to_id = {\"positive\":0, \"negative\" : 1, \"neutral\": 2}\n",
    "id_to_label = {0:\"positive\", 1:\"negative\", 2 : \"neutral\"}\n",
    "\n",
    "\n",
    "### Here we create a training set that can be used to compare across different sizes of training data\n",
    "\n",
    "\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,dataframe):\n",
    "#         self.labels = [label_to_id[label] for label in dataframe[\"Topic\"] if label in [\"politik\", \"maßnahmen\", \"infektion\", \"impfung\", \"lockdown\", \"wirtschaft\", \"lockerung\", \"überblick\"]]\n",
    "        self.labels = [label_to_id[label] for label in dataframe[\"Opinion\"] if label in [\"positive\", \"negative\", \"neutral\"]]\n",
    "        self.texts = [tokenizer(txt, padding =\"max_length\", max_length = 512, truncation=True, return_tensors=\"pt\") for txt in dataframe[\"Comment\"]]\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_labels = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_labels\n",
    "# ### Define test and train set\n",
    "# text_test = text_train[split_index+1:]\n",
    "# text_train = text_train[:split_index]\n",
    "# labels_test = labels_train[split_index+1:]\n",
    "# labels_train = labels_train[:split_index]\n",
    "\n",
    "\n",
    "### Replace label as int\n",
    "for idx, labels in enumerate(data_labels):\n",
    "    data_labels[idx] = label_to_id[labels]\n",
    "\n",
    "\n",
    "class ClassifierText(nn.Module):\n",
    "    def __init__(self, dropout = 0.5):\n",
    "        super(ClassifierText, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-german-cased\")\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # For Topic Article\n",
    "#         self.linear = nn.Linear(768, 8)\n",
    "        # For Topic Comment\n",
    "#         self.linear = nn.Linear(768, 7)\n",
    "        # For Sentiment\n",
    "        self.linear = nn.Linear(768, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, input_id, mask):\n",
    "        _, pooled_output = self.bert(input_ids = input_id, attention_mask = mask, return_dict = False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3010422,
     "status": "ok",
     "timestamp": 1657646012251,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "2mM5QCUMztog",
    "outputId": "e89acb8f-dc52-4e17-f910-90269fd9279b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 400/400 [04:15<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.132             | Train Accuracy:  0.450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:21<00:00,  1.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.127             | Train Accuracy:  0.497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:20<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.122             | Train Accuracy:  0.525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:19<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 4 | Train Loss:  0.117             | Train Accuracy:  0.570\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:19<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 5 | Train Loss:  0.110             | Train Accuracy:  0.601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:20<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 6 | Train Loss:  0.102             | Train Accuracy:  0.649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:20<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 7 | Train Loss:  0.092             | Train Accuracy:  0.699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:19<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 8 | Train Loss:  0.081             | Train Accuracy:  0.753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:19<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 9 | Train Loss:  0.070             | Train Accuracy:  0.805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [04:19<00:00,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 10 | Train Loss:  0.060             | Train Accuracy:  0.841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_data, learning_rate, epochs):\n",
    "\n",
    "    train = Dataset(train_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=8, shuffle=True)\n",
    "\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda:3\" if use_cuda else \"cpu\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input['attention_mask'].to(device)\n",
    "            input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            \n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "            \n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        print(\n",
    "            f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "            | Train Accuracy: {total_acc_train / len(train_data): .3f}'\n",
    "        )    \n",
    "                  \n",
    "EPOCHS = 10\n",
    "model = ClassifierText()\n",
    "LR = 1e-6\n",
    "\n",
    "train(model, df_train, LR, EPOCHS)\n",
    "# for train_index, test_index in folds.split(data_text, data_labels):\n",
    "#     text_train, labels_train  = data_text[train_index], data_labels[train_index]\n",
    "#     text_test, labels_test  = data_text[test_index], data_labels[test_index]\n",
    "\n",
    "#     train_dict = {\"texts\": text_train, \"labels\" : labels_train}\n",
    "#     test_dict = {\"texts\": text_test, \"labels\" : labels_test}              \n",
    "#     train(model, text_train, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16543,
     "status": "ok",
     "timestamp": 1657648805013,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "vd4YdVEwoRRZ",
    "outputId": "ba3309e6-471e-4f70-f1b9-319a8b6c39d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.524\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, test_data):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=8)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "              test_label = test_label.to(device)\n",
    "              mask = test_input['attention_mask'].to(device)\n",
    "              input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "              output = model(input_id, mask)\n",
    "\n",
    "              acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "              \n",
    "              total_acc_test += acc\n",
    "    \n",
    "    print(f'Test Accuracy: {total_acc_test / len(test_data): .3f}')\n",
    "df_test.drop(df_test.loc[df_test[\"Comment\"].isnull()].index, axis = 0, inplace = True)\n",
    "evaluate(model, df_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),\"Model_CommentSentiment.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 183
    },
    "executionInfo": {
     "elapsed": 710,
     "status": "error",
     "timestamp": 1657646715800,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "L27OLuHDcKSJ",
    "outputId": "ce8412c3-d43f-44b1-e1a1-8c3ca39a929e"
   },
   "outputs": [],
   "source": [
    "model = ClassifierText()\n",
    "model.load_state_dict(torch.load(\"Model_ArticleTextFull.pt\"))\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15911,
     "status": "ok",
     "timestamp": 1657648854443,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "BUCrLvWN8QPa",
    "outputId": "3485689b-da3b-4e3f-8f14-0125f2dbe379"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.521\n"
     ]
    }
   ],
   "source": [
    "geladenes_model = model\n",
    "evaluate(geladenes_model, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7401,
     "status": "ok",
     "timestamp": 1657651020014,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "KR4llze89wQA",
    "outputId": "61d29a3e-5a0e-47ca-97e8-af64ddc8199d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.535\n",
      "{'positive': 0.36, 'negative': 0.6708860759493671, 'neutral': 0.49514563106796117}\n",
      "negative    316\n",
      "neutral     309\n",
      "positive    175\n",
      "Name: Opinion, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from numpy.ma.core import argmax\n",
    "from transformers import TextClassificationPipeline\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "if use_cuda:\n",
    "\n",
    "    mein_model = geladenes_model.cuda()\n",
    "\n",
    "# correct_per_topic = {\"politik\" : 0, \"maßnahmen\" : 0, \"infektion\" : 0, \"impfung\": 0, \"lockdown\": 0, \"wirtschaft\":0, \"lockerung\":0, \"überblick\":0}\n",
    "# correct_per_topic = {\"politik\" : 0, \"maßnahmen\" : 0, \"infektion\" : 0, \"impfung\": 0, \"lockdown\": 0, \"wirtschaft\":0, \"lockerung\":0}\n",
    "correct_per_topic = {\"positive\" : 0, \"negative\": 0, \"neutral\" : 0}\n",
    "true = 0\n",
    "with torch.no_grad():\n",
    "\n",
    "  for _, sample in df_test.iterrows():\n",
    "    input_ids = tokenizer(sample[\"Comment\"], return_tensors=\"pt\", truncation=True)[\"input_ids\"].to(device)\n",
    "\n",
    "    masks = tokenizer(sample[\"Comment\"], return_tensors=\"pt\", truncation=True)[\"attention_mask\"].to(device)\n",
    "    \n",
    "    prediction = model( input_ids, masks)\n",
    " \n",
    "    class_pred = np.argmax(prediction.cpu()).item()\n",
    "    \n",
    "#     topic = label_to_id[sample[\"Topic\"]]\n",
    "    topic = label_to_id[sample[\"Opinion\"]]\n",
    "\n",
    "    #print(class_pred, topic)\n",
    "    \n",
    "    \n",
    "    if class_pred == topic:\n",
    "      true+= 1\n",
    "      correct_per_topic[id_to_label[topic]] += 1\n",
    "\n",
    "\n",
    "print(true/len(df_test))\n",
    "\n",
    "# for topic in df_test[\"Topic\"].unique():\n",
    "#   correct_per_topic[topic] /= df_test[\"Topic\"].value_counts()[topic]\n",
    "\n",
    "for sentiment in df_test[\"Opinion\"].unique():\n",
    "  correct_per_topic[sentiment] /= df_test[\"Opinion\"].value_counts()[sentiment]\n",
    "\n",
    "    # prediction = model(mask=masks, input_id=input_ids)\n",
    "\n",
    "    # topic = np.argmax(prediction)\n",
    "\n",
    "    # topic\n",
    "print(correct_per_topic)\n",
    "# print(df_train[\"Topic\"].value_counts())\n",
    "print(df_test[\"Opinion\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1075,
     "status": "ok",
     "timestamp": 1657648113975,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "1wTZyqi0LW68",
    "outputId": "1402fa31-83bd-4cde-8c7a-a41e325f6cd0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neutral     1543\n",
      "negative    1511\n",
      "positive     142\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"Sentiment\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "überblick              995\n",
       "maßnahmen              411\n",
       "wirtschaft             362\n",
       "infektion              342\n",
       "impfung                328\n",
       "lockdown               299\n",
       "politik                242\n",
       "lockerung              164\n",
       "usa                    132\n",
       "reise                  100\n",
       "demos                   90\n",
       "virus                   81\n",
       "maskenpflicht           63\n",
       "arbeit                  62\n",
       "tests                   54\n",
       "krankenhaus             34\n",
       "sport                   33\n",
       "china                   19\n",
       "tönnies                  7\n",
       "masken                   4\n",
       "antikörper               3\n",
       "raus                     3\n",
       "tourismus                3\n",
       "testpflicht              3\n",
       "einreise                 3\n",
       "intesivstationen         2\n",
       "fussball                 2\n",
       "staatshilfen             2\n",
       "homeschooling            1\n",
       "sterberate               1\n",
       "who                      1\n",
       "mobilität                1\n",
       "gewalt                   1\n",
       "grenzkontrollen          1\n",
       "kontaksperre             1\n",
       "steuersenkung            1\n",
       "übersicht                1\n",
       "schließungen             1\n",
       "ÖPNV                     1\n",
       "kinder                   1\n",
       "alkoholverbot            1\n",
       "beherbergungsverbot      1\n",
       "abstand                  1\n",
       "todesfälle               1\n",
       "neutral                  1\n",
       "schließung               1\n",
       "rki                      1\n",
       "RKI                      1\n",
       "auslandshilfe            1\n",
       "sperrstunde              1\n",
       "kinderförderung          1\n",
       "astra                    1\n",
       "pflegebonus              1\n",
       "trump                    1\n",
       "coronahilfe              1\n",
       "kurzarbeit               1\n",
       "verkehr                  1\n",
       "coronahilfen             1\n",
       "hilfspaket               1\n",
       "maßnahmne                1\n",
       "flüge                    1\n",
       "präsenzpflicht           1\n",
       "imfpung                  1\n",
       "USA                      1\n",
       "immunität                1\n",
       "grenzschließung          1\n",
       "einreiseregeln           1\n",
       "coronaleugner            1\n",
       "kontrolle                1\n",
       "überblickl               1\n",
       "querdenker               1\n",
       "grenzkontrolle           1\n",
       "einzelhandel             1\n",
       "belüftungssystem         1\n",
       "Warnapp                  1\n",
       "grenzschliessung         1\n",
       "krankehaus               1\n",
       "negative                 1\n",
       "stufenplan               1\n",
       "schulverlängerung        1\n",
       "präsenzunterricht        1\n",
       "einreiseregel            1\n",
       "inzidenzwert             1\n",
       "ausgangssperre           1\n",
       "medikamente              1\n",
       "testplicht               1\n",
       "schnelltest              1\n",
       "überblick+               1\n",
       "reuse                    1\n",
       "kontakbeschränkung       1\n",
       "maske                    1\n",
       "kontaktverbot            1\n",
       "schuldenbremse           1\n",
       "krise                    1\n",
       "kontaktverfolgung        1\n",
       "kirche                   1\n",
       "grippe                   1\n",
       "Name: Topic_article, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option(\"max_rows\", None)\n",
    "meine_daten = pd.read_csv(\"annotated_data/annotated_data_with_users_and_al_cleaned2x.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"topic_comment\", \"Topic_article\", \"Comment\", \"Method\"], encoding=\"ISO-8859-1\", header=None)\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"intensivstation\", \"Topic_article\"] = \"krankenhaus\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"homeoffice\", \"Topic_article\"] = \"arbeit\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"schule\", \"Topic_article\"] = \"maßnahmen\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"reiseverbot\", \"Topic_article\"] = \"reise\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"quarantäne\", \"Topic_article\"] = \"maßnahmen\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"schulschließung\", \"Topic_article\"] = \"maßnahmen\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"demo\", \"Topic_article\"] = \"demos\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"warnapp\", \"Topic_article\"] = \"maßnahmen\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"intensivstationen\", \"Topic_article\"] = \"krankenhaus\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"lockerungen\", \"Topic_article\"] = \"lockerung\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"konjunktur\", \"Topic_article\"] = \"wirtschaft\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"schulschliessung\", \"Topic_article\"] = \"maßnahmen\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"inzidenz\", \"Topic_article\"] = \"infektion\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"kontaktbeschränkung\", \"Topic_article\"] = \"maßnahmen\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"schnelltests\", \"Topic_article\"] = \"tests\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"verschärfungen\", \"Topic_article\"] = \"maßnahmen\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"intesivstation\", \"Topic_article\"] = \"krankenhaus\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"maskenplficht\", \"Topic_article\"] = \"maskenpflicht\"\n",
    "meine_daten.loc[meine_daten[\"Topic_article\"] == \"fußball\", \"Topic_article\"] = \"sport\"\n",
    "meine_daten = meine_daten[meine_daten.Topic_article != \"XXX\"]\n",
    "meine_daten.to_csv(\"annotated_data/annotated_data_with_users_and_al_cleaned2x.csv\", header=None, index=None)\n",
    "meine_daten[\"Topic_article\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 7787,
     "status": "ok",
     "timestamp": 1657623318552,
     "user": {
      "displayName": "Sven",
      "userId": "16997037369850351210"
     },
     "user_tz": -120
    },
    "id": "F5SMUo_jztoh",
    "outputId": "e2d07260-53a1-43d2-e131-7c73b44ea93c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "politik          755\n",
       "maßnahmen        603\n",
       "impfung          549\n",
       "infektion        424\n",
       "wirtschaft       372\n",
       "lockdown         323\n",
       "reise            127\n",
       "usa              127\n",
       "lockerung        124\n",
       "demos            114\n",
       "virus            105\n",
       "maskenpflicht     95\n",
       "tests             89\n",
       "arbeit            77\n",
       "krankenhaus       42\n",
       "sport             37\n",
       "china             33\n",
       "Column8            1\n",
       "Name: topic_comment, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "volle_daten= pd.read_csv(\"annotated_data/annotated_data_with_users_and_al_cleaned2check.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"topic_comment\", \"Topic_article\", \"Comment\", \"Method\"], encoding=\"ISO-8859-1\", header=None)\n",
    "volle_daten.topic_comment.value_counts()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "eintest.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb0ed09b01ac7e66b2ee8fd1b727dbc61e234ad6836f9832d0e5faf71aa1bb7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
