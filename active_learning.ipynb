{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from scipy.stats import entropy\n",
    "from datasets import load_dataset, Dataset, load_metric\n",
    "import os.path\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES']='1, 2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    evaluation_strategy=\"epoch\"\n",
    "    )\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W&B installed but not logged in. Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "def compute_metrics_accuracy(eval_pred):\n",
    "        logits, labels = eval_pred\n",
    "        predictions = np.argmax(logits, axis=-1)\n",
    "        print(\"Predictions\", predictions, \"Labels\", labels)\n",
    "        return metric.compute(predictions=predictions, references=labels)\n",
    "        \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"oliverguhr/german-sentiment-bert\")\n",
    "trainer = Trainer(model= AutoModelForSequenceClassification.from_pretrained(\"oliverguhr/german-sentiment-bert\"),\n",
    "                args=training_args,\n",
    "                train_dataset=None,\n",
    "                eval_dataset=None,\n",
    "                tokenizer=tokenizer,\n",
    "                data_collator=None,\n",
    "                compute_metrics = compute_metrics_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# trainer = Trainer(model= AutoModelForSequenceClassification.from_pretrained(\"oliverguhr/german-sentiment-bert\"),\n",
    "#                 args=training_args,\n",
    "#                 train_dataset=None,\n",
    "#                 eval_dataset=tokenized_dataset_test,\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 data_collator=data_collator,\n",
    "#                 compute_metrics = compute_metrics_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "## Entropy active learning from https://gist.github.com/Am1n3e/5dda0cfa5443a8fc7e8faf6ddc8a6254\n",
    "\n",
    "def calculate_entropy(logits):\n",
    "    probas = torch.nn.Softmax(dim=1)(torch.from_numpy(logits))\n",
    "    samples_entropy = entropy(probas.transpose(0, 1).cpu())\n",
    "    samples_entropy = torch.from_numpy(samples_entropy)\n",
    "    return samples_entropy\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"texts\"], padding = True, truncation=True)\n",
    "\n",
    "def get_new_sample_active_learning(number_of_comments):\n",
    "\n",
    "    ### We read in the csv that stores all of our already annotated data\n",
    "    df = pd.read_csv(\"annotated_data/annotated_data_with_users.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\",\n",
    "                     \"topic_comment\", \"Topic_article\", \"Comment\", \"Method\"])\n",
    "\n",
    "    already_present_data = 0\n",
    "    with open(\"annotated_data/annotated_data_training.txt\", encoding=\"utf-8\", mode=\"r+\") as y:\n",
    "        for line in y.readlines():\n",
    "            if line != \"\\n\":\n",
    "                already_present_data += 1\n",
    "\n",
    "    ### Check if we already have active learned comments that we annotated\n",
    "    if os.path.isfile('annotated_data/active_learning_comments.csv'):\n",
    "        progress_csv =pd.read_csv(\"annotated_data/active_learning_comments.csv\",names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"topic_comment\", \"Topic_article\", \"Comment\", \"Method\"], encoding=\"utf-8-sig\", header=None)\n",
    "\n",
    "\n",
    "    ### Stores comments as string that we already have in active learning csv\n",
    "    bereits_comments = []\n",
    "    for comment in progress_csv.Comment:\n",
    "        bereits_comments.append(comment)\n",
    "\n",
    "    ### Int which shows how many comments (manual random sampled + active learn sampled) we already have annotated\n",
    "    progress = already_present_data + len(bereits_comments)\n",
    "\n",
    "\n",
    "    ### Load in all scraped comments from web in csv, sliced on [progress: progress + number_of_comments]\n",
    "    alle_kommentare = pd.read_csv(\"shuffled_corona_relevante_kommentare.txt\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"topic_comment\", \"Topic_article\", \"Comment\"], \n",
    "                                    delimiter=\"\\t\", index_col=False, skiprows=progress, nrows=number_of_comments )\n",
    "    \n",
    "\n",
    "\n",
    "    ### Drop all rows that are already present in the active learned csv to we don't annotate twice\n",
    "    alle_kommentare = alle_kommentare.loc[~alle_kommentare[\"Opinion\"].isin(bereits_comments)]\n",
    "    alle_kommentare.reset_index(inplace=True, drop=True)\n",
    "    ### Tokenize texts and get entropy, take topk \n",
    "    texte = {\"texts\" : [x for x in alle_kommentare[\"Opinion\"]]}\n",
    "    texte_ds= Dataset.from_dict(texte)\n",
    "    tokenized_text = texte_ds.map(preprocess_function, batched=True)\n",
    "    entropies = calculate_entropy(trainer.predict(tokenized_text).predictions)\n",
    "    indexes = torch.topk(entropies, int(100)).indices\n",
    "    \n",
    "    # if os.path.isfile('annotated_data/active_learning_comments.csv'):\n",
    "    #     progress_csv =pd.read_csv(\"annotated_data/active_learning_comments.csv\",names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"topic_comment\", \"Topic_article\", \"Comment\"], encoding=\"utf-8-sig\", header=None)\n",
    "    #     print(type(progress_csv))\n",
    "    #     display(progress_csv)\n",
    "    #     for row, index in progress_csv.iterrows():\n",
    "    #         print(row, index)\n",
    "    newdf = pd.DataFrame(columns=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"Topic_comment\", \"Topic_article\", \"Comment\", \"Method\"])\n",
    "    for nummer, x in enumerate(indexes):\n",
    "        satz = texte[\"texts\"][x]\n",
    "        \n",
    "        opinion = input(f\"Opinion --- {satz}\")\n",
    "        print(type(opinion), opinion, opinion not in [\"neutral\", \"positive\", \"negative\", \"exit\"])\n",
    "        while opinion not in [\"neutral\", \"positive\", \"negative\", \"exit\"]:\n",
    "            \n",
    "            opinion = input(f\"Opinion --- {satz}\")    \n",
    "        sentiment = input(f\"Sentiment --- {satz}\")\n",
    "        \n",
    "        while sentiment not in [\"neutral\", \"positive\", \"negative\"]:\n",
    "            sentiment = input(f\"Sentiment --- {satz}\")\n",
    "\n",
    "        \n",
    "        request = requests.get(\"https://www.spiegel.de/wissenschaft/medizin/corona-news-am-samstag-die-wichtigsten-entwicklungen-zu-sars-cov-2-und-covid-19-a-\" +alle_kommentare.loc[int(x), \"ID\"])\n",
    "        soup = BeautifulSoup(request.content, \"html.parser\")\n",
    "        title = soup.find(\"title\").text\n",
    "        subtitle = soup.find(\"meta\", property=\"og:description\")[\"content\"]\n",
    "\n",
    "        topic_article = input(\"Topic Article ------\" +title + \"\\n\" + subtitle + \" \" + alle_kommentare.loc[int(x), \"ID\"])  \n",
    "        topic_comment = input(\"Topic comment\" + satz)\n",
    "\n",
    "\n",
    "        row = [alle_kommentare.loc[alle_kommentare.Opinion == satz]]\n",
    "        newdf = newdf.append(row)\n",
    "        newdf.reset_index(inplace=True, drop=True)\n",
    "        newdf.iloc[nummer,alle_kommentare.columns.get_loc(\"Topic_article\")] = topic_article\n",
    "        newdf.iloc[nummer,alle_kommentare.columns.get_loc(\"topic_comment\")] = topic_comment\n",
    "        newdf.iloc[nummer,alle_kommentare.columns.get_loc(\"Sentiment\")] = sentiment\n",
    "        newdf.iloc[nummer,alle_kommentare.columns.get_loc(\"Comment\")] = satz\n",
    "        newdf.iloc[nummer,alle_kommentare.columns.get_loc(\"Opinion\")] = opinion\n",
    "\n",
    "        print(opinion, sentiment,topic_article, topic_comment)\n",
    "        \n",
    "\n",
    "    newdf = newdf.drop(columns=list(newdf.columns[-2:]))\n",
    "    display(newdf)\n",
    "    newdf.to_csv(\"annotated_data/active_learning_comments.csv\", mode=\"a\", encoding=\"utf-8-sig\", index=False, header=False)\n",
    "            # df.append(line.split(\"\\t\")[0] + \"\\t\" + line.split(\"\\t\")[1] + \"\\t\" + line.split(\"\\t\")[2] + \"\\t\" + line.split(\"\\t\")[3] + \"\\t\" + line.split(\"\\t\")[4] + \"\\t\" + opinion + \"\\t\"\n",
    "            #                       + sentiment + \"\\t\" + klasse1 + \"\\t\" +  klasse2 + \"\\t\" + kommentar + \"\\n\")\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Takes 100 comments with highest entropy from n pool of comments\n",
    "get_new_sample_active_learning(32500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_df = pd.read_csv(\"annotated_data/active_learning_comments.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"Topic_comment\", \"Topic_article\", \"Comment\", \"Method\"])\n",
    "active_learning_df[\"Method\"] = \"manual_al\"\n",
    "active_learning_df.to_csv(\"annotated_data/active_learning_comments.csv\",encoding=\"utf-8-sig\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_learning_df = pd.read_csv(\"annotated_data/active_learning_comments.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"Topic_comment\", \"Topic_article\", \"Comment\", \"Method\"])\n",
    "current_kommentare_df = pd.read_csv(\"annotated_data/annotated_data_with_users_f√ºr_al.csv\", names=[\"ID\", \"Date\", \"Time\", \"Comment Level\", \"Username\", \"Opinion\", \"Sentiment\", \"Topic_comment\", \"Topic_article\", \"Comment\", \"Method\"])\n",
    "full_df = current_kommentare_df.append(active_learning_df)\n",
    "full_df  = full_df.sample(frac=1).reset_index(drop=True)\n",
    "full_df.to_csv(\"annotated_data/annotated_data_with_users_and_al.csv\", encoding=\"utf-8-sig\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb0ed09b01ac7e66b2ee8fd1b727dbc61e234ad6836f9832d0e5faf71aa1bb7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
